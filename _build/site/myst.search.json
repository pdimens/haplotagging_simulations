{"version":"1","records":[{"hierarchy":{"lvl1":"Invesion Simulations"},"type":"lvl1","url":"/simulations","position":0},{"hierarchy":{"lvl1":"Invesion Simulations"},"content":"This document tracks the workflow to simulate the inversions (and SNPs) onto 10 individuals for 5 different depth treatments.\n\nimport os\n\n","type":"content","url":"/simulations","position":1},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Simulating the inversions"},"type":"lvl2","url":"/simulations#simulating-the-inversions","position":2},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Simulating the inversions"},"content":"Prior to simulating the samples, we need to create the inversions themselves, which will be manually modified to meet the treatment goals listed below. Since each chromosome will be a technical replicate of different “conditions”, we need to simulate different inversions on each one, which is more tedious but ultimately worth it.\n\n%%bash\nCONTIGS=(\"2L\" \"2R\" \"3R\" \"3L\")\nSIZES=(\"small\" \"medium\" \"large\" \"xl\")\n\nfor size in \"${SIZES[@]}\"; do\n    for contig in \"${CONTIGS[@]}\"; do\n        harpy simulate inversion -n 5 --max-size 25000 --min-size 1000 -o simulated_data/inversions/${size}/${contig} ${contig}.fasta\n    done\ndone\n\n","type":"content","url":"/simulations#simulating-the-inversions","position":3},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Creating inversion treatments for individuals"},"type":"lvl2","url":"/simulations#creating-inversion-treatments-for-individuals","position":4},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Creating inversion treatments for individuals"},"content":"We need to create the inversion treatments, which is the way the inversions are sized and distributed among the contigs.\nThe design is such that:\n\n2R: all heterozygote\n\n2L: all homozygote\n\n3R: inversions are common\n\n3L: inversions are rare\nThe dictionaries below govern which variants go into which haplotype for each sample, following the formatsample_id : [hap1, hap2]\n\n","type":"content","url":"/simulations#creating-inversion-treatments-for-individuals","position":5},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Small inversions","lvl2":"Creating inversion treatments for individuals"},"type":"lvl3","url":"/simulations#small-inversions","position":6},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Small inversions","lvl2":"Creating inversion treatments for individuals"},"content":"\n\nSMALL = {\n    \"2R\" : {\n        \"sample_01\" : [[1,3,5], [2,4]],\n        \"sample_02\" : [[1,2], [3,4,5]],\n        \"sample_03\" : [[1,5], [2,3,4]],\n        \"sample_04\" : [[1,4,5], [2,3]],\n        \"sample_05\" : [[1,2,3,4,5], []],\n        \"sample_06\" : [[1,2,3,4], [5]],\n        \"sample_07\" : [[1,2,5], [3,4]],\n        \"sample_08\" : [[1,2,4,5], [3]],\n        \"sample_09\" : [[1,3,4,5], [2]],\n        \"sample_10\" : [[1,2,3,5], [4]]\n    },\n    \"2L\" : {\n        \"sample_01\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_02\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_03\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_04\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_05\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_06\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_07\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_08\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_09\" : [[1,2,3,4,5], [1,2,3,4,5]],\n        \"sample_10\" : [[1,2,3,4,5], [1,2,3,4,5]]\n    },\n    \"3R\" : {\n        \"sample_01\" : [[1,3,4,5], [1,3,5]],\n        \"sample_02\" : [[1,3,4,5], [1,3]],\n        \"sample_03\" : [[1,3,4], [1,4]],\n        \"sample_04\" : [[1,2,3], [3]],\n        \"sample_05\" : [[1,2,3], [2,5]],\n        \"sample_06\" : [[1,2,3], [2,3]],\n        \"sample_07\" : [[1,2,3], [2]],\n        \"sample_08\" : [[2,3,5], [2]],\n        \"sample_09\" : [[2,3,4], []],\n        \"sample_10\" : [[2,4,5], [4,5]]\n    },\n    \"3L\" : {\n        \"sample_01\" : [[1], []],\n        \"sample_02\" : [[2], [2]],\n        \"sample_03\" : [[], [3]],\n        \"sample_04\" : [[4], [4]],\n        \"sample_05\" : [[4], []],\n        \"sample_06\" : [[], [1]],\n        \"sample_07\" : [[4], []],\n        \"sample_08\" : [[5], [5]],\n        \"sample_09\" : [[5], [5]],\n        \"sample_10\" : [[5], []]\n    }\n}\n\n","type":"content","url":"/simulations#small-inversions","position":7},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Medium Inversions","lvl2":"Creating inversion treatments for individuals"},"type":"lvl3","url":"/simulations#medium-inversions","position":8},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Medium Inversions","lvl2":"Creating inversion treatments for individuals"},"content":"\n\nMEDIUM = {\n    \"2R\" : {\n        \"sample_01\" : [[1,3], [2,4]],\n        \"sample_02\" : [[1,2], [3,4]],\n        \"sample_03\" : [[1], [2,3,4]],\n        \"sample_04\" : [[1,4], [2,3]],\n        \"sample_05\" : [[1,2,3,4], []],\n        \"sample_06\" : [[1,2,3,4], []],\n        \"sample_07\" : [[1,2], [3,4]],\n        \"sample_08\" : [[1,2,4], [3]],\n        \"sample_09\" : [[1,3,4], [2]],\n        \"sample_10\" : [[1,2,3], [4]]\n    },\n    \"2L\" : {\n        \"sample_01\" : [[1,2,3,4], [1,2,3,4]],\n        \"sample_02\" : [[1,2,3,4], [1,2,3,4]],\n        \"sample_03\" : [[1,2,3,4], [1,2,3,4]],\n        \"sample_04\" : [[1,2,3,4], [1,2,3,4]],\n        \"sample_05\" : [[1,2,3,4], [1,2,3,4]],\n        \"sample_06\" : [[1,2,3,4], [1,2,3,44]],\n        \"sample_09\" : [[1,2,3,4], [1,2,3,4]],\n        \"sample_10\" : [[1,2,3,4], [1,2,3,4]]\n    },\n    \"3R\" : {\n        \"sample_01\" : [[1,3,4], [1,3]],\n        \"sample_02\" : [[1,3,4], [1,3]],\n        \"sample_03\" : [[1,3,4], [1,4]],\n        \"sample_04\" : [[1,2,3], [3]],\n        \"sample_05\" : [[1,2,3], [2]],\n        \"sample_06\" : [[1,2,3], [2,3]],\n        \"sample_07\" : [[1,2,3], [2]],\n        \"sample_08\" : [[2,3], [2]],\n        \"sample_09\" : [[2,3,4], []],\n        \"sample_10\" : [[2,4], [4]]\n    },\n    \"3L\" : {\n        \"sample_01\" : [[1], []],\n        \"sample_02\" : [[2], [2]],\n        \"sample_03\" : [[], [3]],\n        \"sample_04\" : [[4], [4]],\n        \"sample_05\" : [[4], []],\n        \"sample_06\" : [[], [1]],\n        \"sample_07\" : [[4], []],\n        \"sample_08\" : [[], []],\n        \"sample_09\" : [[1], [1]],\n        \"sample_10\" : [[], [3]]\n    }\n}\n\n","type":"content","url":"/simulations#medium-inversions","position":9},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Large Inversions","lvl2":"Creating inversion treatments for individuals"},"type":"lvl3","url":"/simulations#large-inversions","position":10},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Large Inversions","lvl2":"Creating inversion treatments for individuals"},"content":"\n\nLARGE = {\n    \"2R\" : {\n        \"sample_01\" : [[1], [2]],\n        \"sample_02\" : [[], [1,2]],\n        \"sample_03\" : [[1], [2]],\n        \"sample_04\" : [[1], [2]],\n        \"sample_05\" : [[1,2], []],\n        \"sample_06\" : [[], [1,2]],\n        \"sample_07\" : [[2], [1]],\n        \"sample_08\" : [[1,2], []],\n        \"sample_09\" : [[1], [2]],\n        \"sample_10\" : [[1,2], []]\n    },\n    \"2L\" : {\n        \"sample_01\" : [[1,2], [1,2]],\n        \"sample_02\" : [[1,2], [1,2]],\n        \"sample_03\" : [[1,2], [1,2]],\n        \"sample_04\" : [[1,2], [1,2]],\n        \"sample_05\" : [[1,2], [1,2]],\n        \"sample_06\" : [[1,2], [1,2]],\n        \"sample_07\" : [[1,2], [1,2]],\n        \"sample_08\" : [[1,2], [1,2]],\n        \"sample_09\" : [[1,2], [1,2]],\n        \"sample_10\" : [[1,2], [1,2]]\n    },\n    \"3R\" : {\n        \"sample_01\" : [[1], [1]],\n        \"sample_02\" : [[1], [1]],\n        \"sample_03\" : [[1], [2]],\n        \"sample_04\" : [[1,2], []],\n        \"sample_05\" : [[1,2], [1,2]],\n        \"sample_06\" : [[1,2], [2]],\n        \"sample_07\" : [[1], [1,2]],\n        \"sample_08\" : [[2], [2]],\n        \"sample_09\" : [[2], []],\n        \"sample_10\" : [[2], []]\n    },\n    \"3L\" : {\n        \"sample_01\" : [[1], []],\n        \"sample_02\" : [[2], [2]],\n        \"sample_03\" : [[], []],\n        \"sample_04\" : [[], []],\n        \"sample_05\" : [[], []],\n        \"sample_06\" : [[], [1]],\n        \"sample_07\" : [[], []],\n        \"sample_08\" : [[], []],\n        \"sample_09\" : [[1], [1]],\n        \"sample_10\" : [[], []]\n    }\n}       \n\n","type":"content","url":"/simulations#large-inversions","position":11},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"XL Inversions","lvl2":"Creating inversion treatments for individuals"},"type":"lvl3","url":"/simulations#xl-inversions","position":12},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"XL Inversions","lvl2":"Creating inversion treatments for individuals"},"content":"\n\nXL = {\n    \"2R\" : {\n        \"sample_01\" : [[1], []],\n        \"sample_02\" : [[], [1]],\n        \"sample_03\" : [[1], []],\n        \"sample_04\" : [[1], []],\n        \"sample_05\" : [[1], []],\n        \"sample_06\" : [[], [1]],\n        \"sample_07\" : [[], [1]],\n        \"sample_08\" : [[1], []],\n        \"sample_09\" : [[1], []],\n        \"sample_10\" : [[1], []]\n    },\n    \"2L\" : {\n        \"sample_01\" : [[1], [1]],\n        \"sample_02\" : [[1], [1]],\n        \"sample_03\" : [[1], [1]],\n        \"sample_04\" : [[1], [1]],\n        \"sample_05\" : [[1], [1]],\n        \"sample_06\" : [[1], [1]],\n        \"sample_07\" : [[1], [1]],\n        \"sample_08\" : [[1], [1]],\n        \"sample_09\" : [[1], [1]],\n        \"sample_10\" : [[1], [1]]\n    },\n    \"3R\" : {\n        \"sample_01\" : [[1], [1]],\n        \"sample_02\" : [[1], [1]],\n        \"sample_03\" : [[1], []],\n        \"sample_04\" : [[], []],\n        \"sample_05\" : [[1], [1]],\n        \"sample_06\" : [[1], []],\n        \"sample_07\" : [[1], [1]],\n        \"sample_08\" : [[], []],\n        \"sample_09\" : [[], [1]],\n        \"sample_10\" : [[], [1]]\n    },\n    \"3L\" : {\n        \"sample_01\" : [[1], []],\n        \"sample_02\" : [[], []],\n        \"sample_03\" : [[], []],\n        \"sample_04\" : [[], []],\n        \"sample_05\" : [[], []],\n        \"sample_06\" : [[], [1]],\n        \"sample_07\" : [[], []],\n        \"sample_08\" : [[], []],\n        \"sample_09\" : [[1], [1]],\n        \"sample_10\" : [[], []]\n    }\n}\n\n","type":"content","url":"/simulations#xl-inversions","position":13},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Write the inversions"},"type":"lvl2","url":"/simulations#write-the-inversions","position":14},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Write the inversions"},"content":"Using the schema from the dictionaries above, the inversions need to be written to VCF files so that we can simulate them with LRSIM (via harpy)\n\ninv_inventory = {\n    \"small\" : SMALL,\n    \"medium\": MEDIUM,\n    \"large\" : LARGE,\n    \"xl\": XL\n}\n\nfor inv,invdict in inv_inventory.items():\n    with open(f\"simulated_data/inverions/{inv}/inv.{inv}.vcf\", \"r\") as vcf:\n        inversions = {}\n        header = \"\"\n        for line in vcf:\n            if line.startswith(\"#\"):\n                header += line\n            else:\n                inv_split = line.split()\n                contig = inv_split[0]\n                if contig not in inversions:\n                    inversions[contig] = [line]\n                else:\n                    inversions[contig].append(line)\n\n    os.makedirs(f\"simulated_data/inversions/{inv}/samples\", exist_ok = True)\n    samples = {}\n    for contig in invdict:\n        for sample,haps in invdict[contig].items():\n            hap1,hap2 = haps\n            inv_hap1 = [inversions[contig][i-1] for i in hap1]\n            inv_hap2 = [inversions[contig][i-1] for i in hap2]\n\n            if sample not in samples:\n                samples[sample] = {\"hap1\" : inv_hap1, \"hap2\" : inv_hap2}\n            else:\n                samples[sample][\"hap1\"] += inv_hap1\n                samples[sample][\"hap2\"] += inv_hap2\n\n    for sample in samples:\n        with (\n            open(f\"simulated_data/inversions/{inv}/samples/{sample}.{inv}.hap1.vcf\", \"w\") as hap1_vcf,\n            open(f\"simulated_data/inversions/{inv}/samples/{sample}.{inv}.hap2.vcf\", \"w\") as hap2_vcf\n        ):\n            _ = hap1_vcf.write(header)\n            _ = hap1_vcf.write(\"\".join(samples[sample][\"hap1\"]))\n            _ = hap2_vcf.write(header)\n            _ = hap2_vcf.write(\"\".join(samples[sample][\"hap2\"]))\n\n","type":"content","url":"/simulations#write-the-inversions","position":15},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Simulating individual genomes with known inversions","lvl2":"Write the inversions"},"type":"lvl3","url":"/simulations#simulating-individual-genomes-with-known-inversions","position":16},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Simulating individual genomes with known inversions","lvl2":"Write the inversions"},"content":"Now that the inversions have been partitioned for each individual for each size treatment class, they can be simulated for every individual. Since the inversions across chromosomes have been combined into a single VCF file, we no longer need to do this per chromosome (thankfully).\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\n\nfor i in \"${sizes[@]}\"; do\n    for j in $(seq -w 1 1 10); do\n        for hap in $(seq 1 2); do\n            harpy simulate inversion --quiet \\\n                -o simulated_data/inversions/${i}/samples/sample_${j}/hap${hap} \\\n                -v simulated_data/inversions/${i}/samples/sample_${j}.${i}.hap${hap}.vcf dmel_genome/dmel_2_3.fa\n            mv simulated_data/inversions/${i}/samples/sample_${j}/hap${hap}/sim.fasta simulated_data/${i}/samples/sample_${j}/hap${hap}/sample_${j}.hap${hap}.fasta\n            rm -r simulated_data/inversions/${i}/samples/sample_${j}/hap${hap}/logs\n        done\n    done\ndone\n\n","type":"content","url":"/simulations#simulating-individual-genomes-with-known-inversions","position":17},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Simulating SNPs"},"type":"lvl2","url":"/simulations#simulating-snps","position":18},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Simulating SNPs"},"content":"So there is variation beyond just inversions, we need to simulate SNPs as well. First, we’ll simulate 120,000 random SNPs, then using some basic Python wizardry, randomly move those SNPs into haplotype 1 or haplotype 2 for each individual. There will be a 70% chance that a SNP will appear in either haplotype, meaning some might be homozygote, heterozygote, or missing entirely (which is fine). The initial simulation can be done for the entire assembly at once, rather than per-chromosome and/or per size-treatment.\n\n%%bash\nharpy simulate snpindel --prefix all --snp-count 120000 -o simulated_data/snps dmel_genome/dmel_2_3.fa\n\nNow that we have a general set of SNPs, we can randomly shuffle them into haplotypes for each sample.\n\nimport random\nimport os\n\nsamples = [f\"sample_{i:02}\" for i in range(1,11)]\nos.makedirs(\"simulated_data/snps/per_sample\", exist_ok = True)\nrng  = random.Random(69)\nfor sample in samples:\n    with(\n        open(\"simulated_data/snps/all.snp.vcf\", \"r\") as vcf,\n        open(f\"simulated_data/snps/per_sample/{sample}.hap1.snp.vcf\", \"w\") as hap1,\n        open(f\"simulated_data/snps/per_sample/{sample}.hap2.snp.vcf\", \"w\") as hap2\n    ):\n        for line in vcf:\n            if line.startswith(\"#\"):\n                hap1.write(line)\n                hap2.write(line)\n                continue\n            for hap in [hap1,hap2]:\n                if rng.random() < 0.7:\n                    hap.write(line)\n\n\nNow, using the same approach as with the inversions, we can simulate the known SNPs onto the sample genomes. simuG takes a while here, so the commands are echod to a text file that will be consumed by parallel for concurrency.\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\nmkdir -p simulated_data/inversions_snps\nfor size in \"${sizes[@]}\"; do\n    for samp in $(seq -w 1 1 10); do\n        SAMPLE=\"sample_${samp}\"\n        for hap in $(seq 1 2); do\n            FASTA=\"simulated_data/inversions/${size}/samples/${SAMPLE}/${SAMPLE}.hap${hap}.fasta\"\n            SNPS=\"simulated_data/snps/per_sample/${SAMPLE}.hap${hap}.snp.vcf\"\n            echo \"harpy simulate snpindel --quiet --prefix ${SAMPLE}_snpinv.hap${hap} --snp-vcf $SNPS -o simulated_data/inversions_snps/${size}/${SAMPLE}/hap${hap} $FASTA\" >> simulated_data/inversions_snps/sim_snps.sh\n        done\n    done\ndone\n#parallel :::: sim.inversions/inversions_snps/sim_snps.sh\n\nFor some reason, simuG is acting up for manual [known] SNP simulation, so using the VCFs of known SNPs that were created, let’s just manually change the bases. Below is a simple simulate_snps() function to accomplish this.\nNOTE: simuG ended up just taking an unusually long time and the alternative below was not used.\n\nimport gzip\ndef simulate_snps(VCF, FASTA, FASTA_OUT):\n    \"\"\"\n    Read the SNPs from a VCF file, then manually change the snps in FASTA and write to FASTA_OUT\n    \"\"\"\n    # dict format\n    # {chromosome: [position, allele]}\n    snps = {}\n    with open(VCF, \"r\") as vcf:\n        for line in vcf:\n            if line.startswith(\"#\"):\n                continue\n            vcf_fields = line.split()\n            chrm = vcf_fields[0]\n            pos = int(vcf_fields[1])\n            alt = vcf_fields[4]\n            if chrm not in snps:\n                snps[chrm] = []\n            else:\n                snps[chrm].append([pos, alt])\n\n    with open(FASTA, \"r\") as fasta, gzip.open(FASTA_OUT, \"wb\", compresslevel=6) as fasta_out:\n        chrm = None\n        seq = []\n        for line in fasta:\n            if line.startswith(\">\"):\n                fasta_out.write(line.encode(\"utf-8\"))\n                if chrm:\n                    for snp in snps[chrm]:\n                        pos,allele = snp\n                        seq[pos-1] = allele\n                    fasta_out.write((\"\".join(seq) + \"\\n\").encode())\n                    seq = []\n                chrm = line.split()[0].lstrip(\">\")\n            else:\n                seq += list(line.rstrip())\n        # make sure the last chromosome gets modified and written\n        for snp in snps[chrm]:\n            pos,allele = snp\n            seq[pos-1] = allele\n        fasta_out.write((\"\".join(seq) + \"\\n\").encode())\n\nimport os\nfor size in [\"small\", \"medium\", \"large\", \"xl\"]:\n    os.makedirs(f\"simulated_data/inversions_snps2/{size}\", exist_ok = True)\n    for sample in [f\"sample_{i:02}\" for i in range(1,11)]:\n        for hap in [1,2]:\n            SNPS=f\"simulated_data/snps/per_sample/{sample}.hap{hap}.snp.vcf\"\n            FASTA=f\"simulated_data/inversions/{size}/samples/{sample}/{sample}.hap{hap}.fasta\"\n            FASTA_OUT=f\"simulated_data/inversions_snps2/{size}/{sample}.snp_inv.hap{hap}.fasta.gz\"\n            simulate_snps(SNPS, FASTA, FASTA_OUT)\n\nAs a basic housekeeping thing, let’s gzip compress the fasta files and delete the logs and workflow folders to reduce disk footprint.\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\nfor size in \"${sizes[@]}\"; do\n    DIR=\"simulated_data/inversions_snps/${size}\"\n    for samp in $(seq -w 1 1 10); do\n        SAMPLE=\"sample_${samp}\"\n        for hap in $(seq 1 2); do\n            bgzip -c ${DIR}/${SAMPLE}/hap${hap}/${SAMPLE}_snpinv.hap${hap}.fasta > ${DIR}/${SAMPLE}.snp_inv.hap${hap}.fasta.gz\n        done\n        rm -r simulated_data/inversions_snps/${size}/${SAMPLE}/\n    done\ndone\n\n","type":"content","url":"/simulations#simulating-snps","position":19},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Simulate Reads from variant-simulated genomes"},"type":"lvl2","url":"/simulations#simulate-reads-from-variant-simulated-genomes","position":20},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Simulate Reads from variant-simulated genomes"},"content":"Now that each individual for each treatment has SNPs simulated onto them, we can simulate the linked reads from these genomes. Since this is time-consuming, the command will be echo’d to a text file that will be read by parallel to speed things up with concurrency.\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\nmkdir -p simulated_data/linked_reads\nfor size in \"${sizes[@]}\"; do\n    DIR=\"simulated_data/inversions_snps/${size}\"\n    for i in $(seq -w 1 1 10); do\n        SAMPLE=\"sample_${i}\"\n        OUTDIR=\"simulated_data/linked_reads/${size}/${SAMPLE}\"\n        echo \"harpy simulate linkedreads --quiet -o ${OUTDIR} -m 10 -r 0 -p 1000 -n 20 ${DIR}/${SAMPLE}.snp_inv.hap1.fasta.gz ${DIR}/${SAMPLE}.snp_inv.hap2.fasta.gz\" >> simulated_data/linked_reads/sim_reads.sh\n    done\ndone\n#parallel --progress -j 16 :::: simulated_data/linked_reads/sim_reads.sh\n\nOnce again, to minimize the disk footprint, we will combine the forward reads of hap0 and hap1, same with reverse, then delete all the intermediate files harpy created.\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\nfor size in \"${sizes[@]}\"; do\n    OUTDIR=\"simulated_data/linked_reads/${size}/raw\"\n    mkdir -p ${OUTDIR}\n    for i in $(seq -w 1 1 10); do\n        SAMPLE=\"sample_${i}\"\n        DIR=\"simulated_data/linked_reads/${size}/${SAMPLE}\"\n        cat ${DIR}/sim_hap0.R1.fq.gz ${DIR}/sim_hap1.R1.fq.gz > ${OUTDIR}/${SAMPLE}.R1.fq.gz\n        cat ${DIR}/sim_hap0.R2.fq.gz ${DIR}/sim_hap1.R2.fq.gz > ${OUTDIR}/${SAMPLE}.R2.fq.gz\n        rm -r ${DIR}\n    done\ndone\n\n","type":"content","url":"/simulations#simulate-reads-from-variant-simulated-genomes","position":21},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Create Depth (Downsampling) treatments"},"type":"lvl2","url":"/simulations#create-depth-downsampling-treatments","position":22},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Create Depth (Downsampling) treatments"},"content":"Now that the reads are created, we need to downsample them to specific depths:\n\n0.5X\n\n2X\n\n5X\n\n10X\n\n20X\n\nThis will be achieved using seqtk, but first it makes sense to do a cursory QC on the reads, then subsample those.\n\n","type":"content","url":"/simulations#create-depth-downsampling-treatments","position":23},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Sample QC","lvl2":"Create Depth (Downsampling) treatments"},"type":"lvl3","url":"/simulations#sample-qc","position":24},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Sample QC","lvl2":"Create Depth (Downsampling) treatments"},"content":"These samples are “real”, so they don’t have adapters or most other weird stuff that would require removal. However, to do due diligence, we need to QC them anyway because the simulations also include sequencer error, possible poly-G tails, and we need to truncate read 1 by 75 base pairs to mirror the read lengths of real haplotagging data, where the barcodes, adapters, etc occupy 75bp of read 1.\n\nread 1: 75bp (150bp - 75bp)\n\nread 2: 150bp\n\ntotal paired-end length: 225bp\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\nfor size in \"${sizes[@]}\"; do\n    DIR=\"simulated_data/linked_reads/${size}/raw\"\n    OUTDIR=\"simulated_data/linked_reads/${size}/qc\"\n    harpy qc --quiet -a auto -m 75 -t 10 -o $OUTDIR -x \"--max_len2 150\" $DIR\ndone\n\n","type":"content","url":"/simulations#sample-qc","position":25},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Depth Downsampling","lvl2":"Create Depth (Downsampling) treatments"},"type":"lvl3","url":"/simulations#depth-downsampling","position":26},{"hierarchy":{"lvl1":"Invesion Simulations","lvl3":"Depth Downsampling","lvl2":"Create Depth (Downsampling) treatments"},"content":"We need a random seed set identically for all samples and depth-treatments so that each “sample” of lower treatment is effectively a subsample of the treatment immediately above it (i.e. 2X is a subset of 5X, which is a subset of 10X, etc.). This means we are testing for “less of the same data” rather than a completely random set of reads for each depth-treatment. A way of accomplishing this is some basic math:\n\nthere are 108,990,206 bp in 4-chrom Dmel assembly used to simulate the data\n\n484,401 PE reads @ 225 bp (total) makes 1X\n\nSo we can just use simple arithmetic to know exactly how many reads we need\n\nEach sample should have its own random seed so we aren’t inadvertantly choosing the same reads across samples. The linked-read simulation has a randomness element when sampling sequences, but it’s better to play it safe here too.\n\nThis is once again parallelized because doing it serially is prohibitively slow\n\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\n# 108,990,206 bp in 4-chrom Dmel assembly\n# 484,401 PE reads @ 225bp (total) makes 1X\n\ndepths = {\n    \"05X\": 484401 // 2,\n    \"2X\": 484401 * 2,\n    \"5X\": 484401* 5,\n    \"10X\": 484401 * 10,\n    \"20X\": 484401 * 20\n}\n\nwith ThreadPoolExecutor(max_workers=20) as executor:\n    for size in [\"small\", \"medium\", \"large\", \"xl\"]:\n        os.makedirs(f\"simulated_data/linked_reads/{size}/depths\", exist_ok = True)\n        DIR = f\"simulated_data/linked_reads/{size}/qc\"\n        OUTDIR = f\"simulated_data/linked_reads/{size}/depths\"\n        for num in range(1,11):\n            SAMPLE=f\"sample_{num:02}\"\n            SEED=num\n            for depth,val in depths.items():\n                executor.submit(os.system, f\"seqtk sample -s{SEED} {DIR}/{SAMPLE}.R1.fq.gz {val} | gzip > {OUTDIR}/{SAMPLE}.{depth}.R1.fq.gz\")\n                executor.submit(os.system, f\"seqtk sample -s{SEED} {DIR}/{SAMPLE}.R2.fq.gz {val} | gzip > {OUTDIR}/{SAMPLE}.{depth}.R2.fq.gz\")\n\n","type":"content","url":"/simulations#depth-downsampling","position":27},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Aligning sequences"},"type":"lvl2","url":"/simulations#aligning-sequences","position":28},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Aligning sequences"},"content":"With the simulated data finally ready and QC’d, we can now align them onto the original (unmodified) 4-chromosome D. melanogaster assembly.\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\nfor size in \"${sizes[@]}\"; do\n    DIR=\"simulated_data/linked_reads/${size}/depths\"\n    OUTDIR=\"simulated_data/alignments/${size}/\"\n    harpy align bwa --skip-reports -t 22 -o $OUTDIR -g dmel_genome/dmel_2_3.fa $DIR\ndone\n\n","type":"content","url":"/simulations#aligning-sequences","position":29},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Calling SNPs"},"type":"lvl2","url":"/simulations#calling-snps","position":30},{"hierarchy":{"lvl1":"Invesion Simulations","lvl2":"Calling SNPs"},"content":"After aligning, we need to call SNPs. This needs to be done per size class and per depth, since the Harpy snp calling process calls for all samples supplied at once and the output is a single BCF file of the SNPs for all the supplied samples.\n\n%%bash\nsizes=(\"small\" \"medium\" \"large\" \"xl\")\ndepths=(\"05X\" \"2X\" \"5X\" \"10X\" \"20X\")\n\nfor size in \"${sizes[@]}\"; do\n    DIR=\"simulated_data/alignments/${size}/\"\n    for depth in \"${depths[@]}\"; do\n        OUTDIR=\"simulated_data/snps/${size}/${depth}\"\n        harpy snp mpileup --skip-reports -r 1000000 -t 20 -o ${OUTDIR} -g dmel_genome/dmel_2_3.fa ${DIR}/sample_*.${depth}.bam\n    done\ndone","type":"content","url":"/simulations#calling-snps","position":31}]}