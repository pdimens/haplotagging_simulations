{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invesion Simulations\n",
    "This document tracks the workflow to simulate the inversions (and SNPs) onto 10 individuals for 5 different depth treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the inversions\n",
    "Prior to simulating the samples, we need to create the inversions themselves, which will be manually modified to meet the treatment goals listed below. Since each chromosome will be a technical replicate of different \"conditions\", we need to simulate different inversions on each one, which is more tedious but ultimately worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "CONTIGS=(\"2L\" \"2R\" \"3R\" \"3L\")\n",
    "SIZES=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "\n",
    "for size in \"${SIZES[@]}\"; do\n",
    "    for contig in \"${CONTIGS[@]}\"; do\n",
    "        harpy simulate inversion -n 5 --max-size 25000 --min-size 1000 -o simulated_data/inversions/${size}/${contig} ${contig}.fasta\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating inversion treatments for individuals\n",
    "We need to create the inversion treatments, which is the way the inversions are sized and distributed among the contigs.\n",
    "The design is such that:\n",
    "- 2R: all heterozygote\n",
    "- 2L: all homozygote\n",
    "- 3R: inversions are common\n",
    "- 3L: inversions are rare\n",
    "The dictionaries below govern which variants go into which haplotype for each sample, following the format\n",
    "```python\n",
    "sample_id : [hap1, hap2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL = {\n",
    "    \"2R\" : {\n",
    "        \"sample_01\" : [[1,3,5], [2,4]],\n",
    "        \"sample_02\" : [[1,2], [3,4,5]],\n",
    "        \"sample_03\" : [[1,5], [2,3,4]],\n",
    "        \"sample_04\" : [[1,4,5], [2,3]],\n",
    "        \"sample_05\" : [[1,2,3,4,5], []],\n",
    "        \"sample_06\" : [[1,2,3,4], [5]],\n",
    "        \"sample_07\" : [[1,2,5], [3,4]],\n",
    "        \"sample_08\" : [[1,2,4,5], [3]],\n",
    "        \"sample_09\" : [[1,3,4,5], [2]],\n",
    "        \"sample_10\" : [[1,2,3,5], [4]]\n",
    "    },\n",
    "    \"2L\" : {\n",
    "        \"sample_01\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_02\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_03\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_04\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_05\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_06\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_07\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_08\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_09\" : [[1,2,3,4,5], [1,2,3,4,5]],\n",
    "        \"sample_10\" : [[1,2,3,4,5], [1,2,3,4,5]]\n",
    "    },\n",
    "    \"3R\" : {\n",
    "        \"sample_01\" : [[1,3,4,5], [1,3,5]],\n",
    "        \"sample_02\" : [[1,3,4,5], [1,3]],\n",
    "        \"sample_03\" : [[1,3,4], [1,4]],\n",
    "        \"sample_04\" : [[1,2,3], [3]],\n",
    "        \"sample_05\" : [[1,2,3], [2,5]],\n",
    "        \"sample_06\" : [[1,2,3], [2,3]],\n",
    "        \"sample_07\" : [[1,2,3], [2]],\n",
    "        \"sample_08\" : [[2,3,5], [2]],\n",
    "        \"sample_09\" : [[2,3,4], []],\n",
    "        \"sample_10\" : [[2,4,5], [4,5]]\n",
    "    },\n",
    "    \"3L\" : {\n",
    "        \"sample_01\" : [[1], []],\n",
    "        \"sample_02\" : [[2], [2]],\n",
    "        \"sample_03\" : [[], [3]],\n",
    "        \"sample_04\" : [[4], [4]],\n",
    "        \"sample_05\" : [[4], []],\n",
    "        \"sample_06\" : [[], [1]],\n",
    "        \"sample_07\" : [[4], []],\n",
    "        \"sample_08\" : [[5], [5]],\n",
    "        \"sample_09\" : [[5], [5]],\n",
    "        \"sample_10\" : [[5], []]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIUM = {\n",
    "    \"2R\" : {\n",
    "        \"sample_01\" : [[1,3], [2,4]],\n",
    "        \"sample_02\" : [[1,2], [3,4]],\n",
    "        \"sample_03\" : [[1], [2,3,4]],\n",
    "        \"sample_04\" : [[1,4], [2,3]],\n",
    "        \"sample_05\" : [[1,2,3,4], []],\n",
    "        \"sample_06\" : [[1,2,3,4], []],\n",
    "        \"sample_07\" : [[1,2], [3,4]],\n",
    "        \"sample_08\" : [[1,2,4], [3]],\n",
    "        \"sample_09\" : [[1,3,4], [2]],\n",
    "        \"sample_10\" : [[1,2,3], [4]]\n",
    "    },\n",
    "    \"2L\" : {\n",
    "        \"sample_01\" : [[1,2,3,4], [1,2,3,4]],\n",
    "        \"sample_02\" : [[1,2,3,4], [1,2,3,4]],\n",
    "        \"sample_03\" : [[1,2,3,4], [1,2,3,4]],\n",
    "        \"sample_04\" : [[1,2,3,4], [1,2,3,4]],\n",
    "        \"sample_05\" : [[1,2,3,4], [1,2,3,4]],\n",
    "        \"sample_06\" : [[1,2,3,4], [1,2,3,44]],\n",
    "        \"sample_09\" : [[1,2,3,4], [1,2,3,4]],\n",
    "        \"sample_10\" : [[1,2,3,4], [1,2,3,4]]\n",
    "    },\n",
    "    \"3R\" : {\n",
    "        \"sample_01\" : [[1,3,4], [1,3]],\n",
    "        \"sample_02\" : [[1,3,4], [1,3]],\n",
    "        \"sample_03\" : [[1,3,4], [1,4]],\n",
    "        \"sample_04\" : [[1,2,3], [3]],\n",
    "        \"sample_05\" : [[1,2,3], [2]],\n",
    "        \"sample_06\" : [[1,2,3], [2,3]],\n",
    "        \"sample_07\" : [[1,2,3], [2]],\n",
    "        \"sample_08\" : [[2,3], [2]],\n",
    "        \"sample_09\" : [[2,3,4], []],\n",
    "        \"sample_10\" : [[2,4], [4]]\n",
    "    },\n",
    "    \"3L\" : {\n",
    "        \"sample_01\" : [[1], []],\n",
    "        \"sample_02\" : [[2], [2]],\n",
    "        \"sample_03\" : [[], [3]],\n",
    "        \"sample_04\" : [[4], [4]],\n",
    "        \"sample_05\" : [[4], []],\n",
    "        \"sample_06\" : [[], [1]],\n",
    "        \"sample_07\" : [[4], []],\n",
    "        \"sample_08\" : [[], []],\n",
    "        \"sample_09\" : [[1], [1]],\n",
    "        \"sample_10\" : [[], [3]]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE = {\n",
    "    \"2R\" : {\n",
    "        \"sample_01\" : [[1], [2]],\n",
    "        \"sample_02\" : [[], [1,2]],\n",
    "        \"sample_03\" : [[1], [2]],\n",
    "        \"sample_04\" : [[1], [2]],\n",
    "        \"sample_05\" : [[1,2], []],\n",
    "        \"sample_06\" : [[], [1,2]],\n",
    "        \"sample_07\" : [[2], [1]],\n",
    "        \"sample_08\" : [[1,2], []],\n",
    "        \"sample_09\" : [[1], [2]],\n",
    "        \"sample_10\" : [[1,2], []]\n",
    "    },\n",
    "    \"2L\" : {\n",
    "        \"sample_01\" : [[1,2], [1,2]],\n",
    "        \"sample_02\" : [[1,2], [1,2]],\n",
    "        \"sample_03\" : [[1,2], [1,2]],\n",
    "        \"sample_04\" : [[1,2], [1,2]],\n",
    "        \"sample_05\" : [[1,2], [1,2]],\n",
    "        \"sample_06\" : [[1,2], [1,2]],\n",
    "        \"sample_07\" : [[1,2], [1,2]],\n",
    "        \"sample_08\" : [[1,2], [1,2]],\n",
    "        \"sample_09\" : [[1,2], [1,2]],\n",
    "        \"sample_10\" : [[1,2], [1,2]]\n",
    "    },\n",
    "    \"3R\" : {\n",
    "        \"sample_01\" : [[1], [1]],\n",
    "        \"sample_02\" : [[1], [1]],\n",
    "        \"sample_03\" : [[1], [2]],\n",
    "        \"sample_04\" : [[1,2], []],\n",
    "        \"sample_05\" : [[1,2], [1,2]],\n",
    "        \"sample_06\" : [[1,2], [2]],\n",
    "        \"sample_07\" : [[1], [1,2]],\n",
    "        \"sample_08\" : [[2], [2]],\n",
    "        \"sample_09\" : [[2], []],\n",
    "        \"sample_10\" : [[2], []]\n",
    "    },\n",
    "    \"3L\" : {\n",
    "        \"sample_01\" : [[1], []],\n",
    "        \"sample_02\" : [[2], [2]],\n",
    "        \"sample_03\" : [[], []],\n",
    "        \"sample_04\" : [[], []],\n",
    "        \"sample_05\" : [[], []],\n",
    "        \"sample_06\" : [[], [1]],\n",
    "        \"sample_07\" : [[], []],\n",
    "        \"sample_08\" : [[], []],\n",
    "        \"sample_09\" : [[1], [1]],\n",
    "        \"sample_10\" : [[], []]\n",
    "    }\n",
    "}       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XL Inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL = {\n",
    "    \"2R\" : {\n",
    "        \"sample_01\" : [[1], []],\n",
    "        \"sample_02\" : [[], [1]],\n",
    "        \"sample_03\" : [[1], []],\n",
    "        \"sample_04\" : [[1], []],\n",
    "        \"sample_05\" : [[1], []],\n",
    "        \"sample_06\" : [[], [1]],\n",
    "        \"sample_07\" : [[], [1]],\n",
    "        \"sample_08\" : [[1], []],\n",
    "        \"sample_09\" : [[1], []],\n",
    "        \"sample_10\" : [[1], []]\n",
    "    },\n",
    "    \"2L\" : {\n",
    "        \"sample_01\" : [[1], [1]],\n",
    "        \"sample_02\" : [[1], [1]],\n",
    "        \"sample_03\" : [[1], [1]],\n",
    "        \"sample_04\" : [[1], [1]],\n",
    "        \"sample_05\" : [[1], [1]],\n",
    "        \"sample_06\" : [[1], [1]],\n",
    "        \"sample_07\" : [[1], [1]],\n",
    "        \"sample_08\" : [[1], [1]],\n",
    "        \"sample_09\" : [[1], [1]],\n",
    "        \"sample_10\" : [[1], [1]]\n",
    "    },\n",
    "    \"3R\" : {\n",
    "        \"sample_01\" : [[1], [1]],\n",
    "        \"sample_02\" : [[1], [1]],\n",
    "        \"sample_03\" : [[1], []],\n",
    "        \"sample_04\" : [[], []],\n",
    "        \"sample_05\" : [[1], [1]],\n",
    "        \"sample_06\" : [[1], []],\n",
    "        \"sample_07\" : [[1], [1]],\n",
    "        \"sample_08\" : [[], []],\n",
    "        \"sample_09\" : [[], [1]],\n",
    "        \"sample_10\" : [[], [1]]\n",
    "    },\n",
    "    \"3L\" : {\n",
    "        \"sample_01\" : [[1], []],\n",
    "        \"sample_02\" : [[], []],\n",
    "        \"sample_03\" : [[], []],\n",
    "        \"sample_04\" : [[], []],\n",
    "        \"sample_05\" : [[], []],\n",
    "        \"sample_06\" : [[], [1]],\n",
    "        \"sample_07\" : [[], []],\n",
    "        \"sample_08\" : [[], []],\n",
    "        \"sample_09\" : [[1], [1]],\n",
    "        \"sample_10\" : [[], []]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the inversions\n",
    "Using the schema from the dictionaries above, the inversions need to be written to VCF files so that we can simulate them with LRSIM (via harpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_inventory = {\n",
    "    \"small\" : SMALL,\n",
    "    \"medium\": MEDIUM,\n",
    "    \"large\" : LARGE,\n",
    "    \"xl\": XL\n",
    "}\n",
    "\n",
    "for inv,invdict in inv_inventory.items():\n",
    "    with open(f\"simulated_data/inverions/{inv}/inv.{inv}.vcf\", \"r\") as vcf:\n",
    "        inversions = {}\n",
    "        header = \"\"\n",
    "        for line in vcf:\n",
    "            if line.startswith(\"#\"):\n",
    "                header += line\n",
    "            else:\n",
    "                inv_split = line.split()\n",
    "                contig = inv_split[0]\n",
    "                if contig not in inversions:\n",
    "                    inversions[contig] = [line]\n",
    "                else:\n",
    "                    inversions[contig].append(line)\n",
    "\n",
    "    os.makedirs(f\"simulated_data/inversions/{inv}/samples\", exist_ok = True)\n",
    "    samples = {}\n",
    "    for contig in invdict:\n",
    "        for sample,haps in invdict[contig].items():\n",
    "            hap1,hap2 = haps\n",
    "            inv_hap1 = [inversions[contig][i-1] for i in hap1]\n",
    "            inv_hap2 = [inversions[contig][i-1] for i in hap2]\n",
    "\n",
    "            if sample not in samples:\n",
    "                samples[sample] = {\"hap1\" : inv_hap1, \"hap2\" : inv_hap2}\n",
    "            else:\n",
    "                samples[sample][\"hap1\"] += inv_hap1\n",
    "                samples[sample][\"hap2\"] += inv_hap2\n",
    "\n",
    "    for sample in samples:\n",
    "        with (\n",
    "            open(f\"simulated_data/inversions/{inv}/samples/{sample}.{inv}.hap1.vcf\", \"w\") as hap1_vcf,\n",
    "            open(f\"simulated_data/inversions/{inv}/samples/{sample}.{inv}.hap2.vcf\", \"w\") as hap2_vcf\n",
    "        ):\n",
    "            _ = hap1_vcf.write(header)\n",
    "            _ = hap1_vcf.write(\"\".join(samples[sample][\"hap1\"]))\n",
    "            _ = hap2_vcf.write(header)\n",
    "            _ = hap2_vcf.write(\"\".join(samples[sample][\"hap2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating individual genomes with known inversions\n",
    "Now that the inversions have been partitioned for each individual for each size treatment class, they can be simulated for every individual. Since the inversions across chromosomes have been combined into a single VCF file, we no longer need to do this per chromosome (thankfully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "\n",
    "for i in \"${sizes[@]}\"; do\n",
    "    for j in $(seq -w 1 1 10); do\n",
    "        for hap in $(seq 1 2); do\n",
    "            harpy simulate inversion --quiet \\\n",
    "                -o simulated_data/inversions/${i}/samples/sample_${j}/hap${hap} \\\n",
    "                -v simulated_data/inversions/${i}/samples/sample_${j}.${i}.hap${hap}.vcf dmel_genome/dmel_2_3.fa\n",
    "            mv simulated_data/inversions/${i}/samples/sample_${j}/hap${hap}/sim.fasta simulated_data/${i}/samples/sample_${j}/hap${hap}/sample_${j}.hap${hap}.fasta\n",
    "            rm -r simulated_data/inversions/${i}/samples/sample_${j}/hap${hap}/logs\n",
    "        done\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating SNPs\n",
    "So there is variation beyond just inversions, we need to simulate SNPs as well. First, we'll simulate 120,000 random SNPs, then using some basic Python wizardry, randomly move those SNPs into haplotype 1 or haplotype 2 for each individual. There will be a 70% chance that a SNP will appear in either haplotype, meaning some might be homozygote, heterozygote, or missing entirely (which is fine). The initial simulation can be done for the entire assembly at once, rather than per-chromosome and/or per size-treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "harpy simulate snpindel --prefix all --snp-count 120000 -o simulated_data/snps dmel_genome/dmel_2_3.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general set of SNPs, we can randomly shuffle them into haplotypes for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "samples = [f\"sample_{i:02}\" for i in range(1,11)]\n",
    "os.makedirs(\"simulated_data/snps/per_sample\", exist_ok = True)\n",
    "rng  = random.Random(69)\n",
    "for sample in samples:\n",
    "    with(\n",
    "        open(\"simulated_data/snps/all.snp.vcf\", \"r\") as vcf,\n",
    "        open(f\"simulated_data/snps/per_sample/{sample}.hap1.snp.vcf\", \"w\") as hap1,\n",
    "        open(f\"simulated_data/snps/per_sample/{sample}.hap2.snp.vcf\", \"w\") as hap2\n",
    "    ):\n",
    "        for line in vcf:\n",
    "            if line.startswith(\"#\"):\n",
    "                hap1.write(line)\n",
    "                hap2.write(line)\n",
    "                continue\n",
    "            for hap in [hap1,hap2]:\n",
    "                if rng.random() < 0.7:\n",
    "                    hap.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the same approach as with the inversions, we can simulate the _known_ SNPs onto the sample genomes. simuG takes a while here, so the commands are `echo`d to a text file that will be consumed by `parallel` for concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "mkdir -p simulated_data/inversions_snps\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    for samp in $(seq -w 1 1 10); do\n",
    "        SAMPLE=\"sample_${samp}\"\n",
    "        for hap in $(seq 1 2); do\n",
    "            FASTA=\"simulated_data/inversions/${size}/samples/${SAMPLE}/${SAMPLE}.hap${hap}.fasta\"\n",
    "            SNPS=\"simulated_data/snps/per_sample/${SAMPLE}.hap${hap}.snp.vcf\"\n",
    "            echo \"harpy simulate snpindel --quiet --prefix ${SAMPLE}_snpinv.hap${hap} --snp-vcf $SNPS -o simulated_data/inversions_snps/${size}/${SAMPLE}/hap${hap} $FASTA\" >> simulated_data/inversions_snps/sim_snps.sh\n",
    "        done\n",
    "    done\n",
    "done\n",
    "#parallel :::: sim.inversions/inversions_snps/sim_snps.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "For some reason, simuG is acting up for manual [known] SNP simulation, so using the VCFs of known SNPs that were created, let's just manually change the bases. Below is a simple `simulate_snps()` function to accomplish this. \n",
    "NOTE: simuG ended up just taking an unusually long time and the alternative below was not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def simulate_snps(VCF, FASTA, FASTA_OUT):\n",
    "    \"\"\"\n",
    "    Read the SNPs from a VCF file, then manually change the snps in FASTA and write to FASTA_OUT\n",
    "    \"\"\"\n",
    "    # dict format\n",
    "    # {chromosome: [position, allele]}\n",
    "    snps = {}\n",
    "    with open(VCF, \"r\") as vcf:\n",
    "        for line in vcf:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            vcf_fields = line.split()\n",
    "            chrm = vcf_fields[0]\n",
    "            pos = int(vcf_fields[1])\n",
    "            alt = vcf_fields[4]\n",
    "            if chrm not in snps:\n",
    "                snps[chrm] = []\n",
    "            else:\n",
    "                snps[chrm].append([pos, alt])\n",
    "\n",
    "    with open(FASTA, \"r\") as fasta, gzip.open(FASTA_OUT, \"wb\", compresslevel=6) as fasta_out:\n",
    "        chrm = None\n",
    "        seq = []\n",
    "        for line in fasta:\n",
    "            if line.startswith(\">\"):\n",
    "                fasta_out.write(line.encode(\"utf-8\"))\n",
    "                if chrm:\n",
    "                    for snp in snps[chrm]:\n",
    "                        pos,allele = snp\n",
    "                        seq[pos-1] = allele\n",
    "                    fasta_out.write((\"\".join(seq) + \"\\n\").encode())\n",
    "                    seq = []\n",
    "                chrm = line.split()[0].lstrip(\">\")\n",
    "            else:\n",
    "                seq += list(line.rstrip())\n",
    "        # make sure the last chromosome gets modified and written\n",
    "        for snp in snps[chrm]:\n",
    "            pos,allele = snp\n",
    "            seq[pos-1] = allele\n",
    "        fasta_out.write((\"\".join(seq) + \"\\n\").encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for size in [\"small\", \"medium\", \"large\", \"xl\"]:\n",
    "    os.makedirs(f\"simulated_data/inversions_snps2/{size}\", exist_ok = True)\n",
    "    for sample in [f\"sample_{i:02}\" for i in range(1,11)]:\n",
    "        for hap in [1,2]:\n",
    "            SNPS=f\"simulated_data/snps/per_sample/{sample}.hap{hap}.snp.vcf\"\n",
    "            FASTA=f\"simulated_data/inversions/{size}/samples/{sample}/{sample}.hap{hap}.fasta\"\n",
    "            FASTA_OUT=f\"simulated_data/inversions_snps2/{size}/{sample}.snp_inv.hap{hap}.fasta.gz\"\n",
    "            simulate_snps(SNPS, FASTA, FASTA_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a basic housekeeping thing, let's gzip compress the fasta files and delete the `logs` and `workflow` folders to reduce disk footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    DIR=\"simulated_data/inversions_snps/${size}\"\n",
    "    for samp in $(seq -w 1 1 10); do\n",
    "        SAMPLE=\"sample_${samp}\"\n",
    "        for hap in $(seq 1 2); do\n",
    "            bgzip -c ${DIR}/${SAMPLE}/hap${hap}/${SAMPLE}_snpinv.hap${hap}.fasta > ${DIR}/${SAMPLE}.snp_inv.hap${hap}.fasta.gz\n",
    "        done\n",
    "        rm -r simulated_data/inversions_snps/${size}/${SAMPLE}/\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Reads from variant-simulated genomes\n",
    "Now that each individual for each treatment has SNPs simulated onto them, we can simulate the linked reads from these genomes. Since this is time-consuming, the command will be `echo`'d to a text file that will be read by `parallel` to speed things up with concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "mkdir -p simulated_data/linked_reads\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    DIR=\"simulated_data/inversions_snps/${size}\"\n",
    "    for i in $(seq -w 1 1 10); do\n",
    "        SAMPLE=\"sample_${i}\"\n",
    "        OUTDIR=\"simulated_data/linked_reads/${size}/${SAMPLE}\"\n",
    "        echo \"harpy simulate linkedreads --quiet -o ${OUTDIR} -m 10 -r 0 -p 1000 -n 20 ${DIR}/${SAMPLE}.snp_inv.hap1.fasta.gz ${DIR}/${SAMPLE}.snp_inv.hap2.fasta.gz\" >> simulated_data/linked_reads/sim_reads.sh\n",
    "    done\n",
    "done\n",
    "#parallel --progress -j 16 :::: simulated_data/linked_reads/sim_reads.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, to minimize the disk footprint, we will combine the forward reads of `hap0` and `hap1`, same with reverse, then delete all the intermediate files harpy created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    OUTDIR=\"simulated_data/linked_reads/${size}/raw\"\n",
    "    mkdir -p ${OUTDIR}\n",
    "    for i in $(seq -w 1 1 10); do\n",
    "        SAMPLE=\"sample_${i}\"\n",
    "        DIR=\"simulated_data/linked_reads/${size}/${SAMPLE}\"\n",
    "        cat ${DIR}/sim_hap0.R1.fq.gz ${DIR}/sim_hap1.R1.fq.gz > ${OUTDIR}/${SAMPLE}.R1.fq.gz\n",
    "        cat ${DIR}/sim_hap0.R2.fq.gz ${DIR}/sim_hap1.R2.fq.gz > ${OUTDIR}/${SAMPLE}.R2.fq.gz\n",
    "        rm -r ${DIR}\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Depth (Downsampling) treatments\n",
    "Now that the reads are created, we need to downsample them to specific depths:\n",
    "- 0.5X\n",
    "- 2X\n",
    "- 5X\n",
    "- 10X\n",
    "- 20X\n",
    "\n",
    "This will be achieved using `seqtk`, but first it makes sense to do a cursory QC on the reads, then subsample those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample QC\n",
    "These samples are \"real\", so they don't have adapters or most other weird stuff that would require removal. However, to do due diligence, we need to QC them anyway because the simulations also include sequencer error, possible poly-G tails, and we need to truncate read 1 by 75 base pairs to mirror the read lengths of real haplotagging data, where the barcodes, adapters, etc occupy 75bp of read 1.\n",
    "- read 1: `75bp` (`150bp` - `75bp`)\n",
    "- read 2: `150bp`\n",
    "- total paired-end length: `225bp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    DIR=\"simulated_data/linked_reads/${size}/raw\"\n",
    "    OUTDIR=\"simulated_data/linked_reads/${size}/qc\"\n",
    "    harpy qc --quiet -a auto -m 75 -t 10 -o $OUTDIR -x \"--max_len2 150\" $DIR\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Downsampling\n",
    " We need a random seed set identically for all samples and depth-treatments so that each \"sample\" of lower treatment is effectively a subsample of the treatment immediately above it (i.e. 2X is a subset of 5X, which is a subset of 10X, etc.). This means we are testing for \"less of the same data\" rather than a completely random set of reads for each depth-treatment. A way of accomplishing this is some basic math:\n",
    " - there are `108,990,206` bp in 4-chrom Dmel assembly used to simulate the data\n",
    " - `484,401` PE reads @ `225` bp (total) makes 1X\n",
    "\n",
    " So we can just use simple arithmetic to know exactly how many reads we need\n",
    " \n",
    " Each sample should have its own random seed so we aren't inadvertantly choosing the same reads across samples. The linked-read simulation has a randomness element when sampling sequences, but it's better to play it safe here too.\n",
    "\n",
    "This is once again parallelized because doing it serially is prohibitively slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 108,990,206 bp in 4-chrom Dmel assembly\n",
    "# 484,401 PE reads @ 225bp (total) makes 1X\n",
    "\n",
    "depths = {\n",
    "    \"05X\": 484401 // 2,\n",
    "    \"2X\": 484401 * 2,\n",
    "    \"5X\": 484401* 5,\n",
    "    \"10X\": 484401 * 10,\n",
    "    \"20X\": 484401 * 20\n",
    "}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    for size in [\"small\", \"medium\", \"large\", \"xl\"]:\n",
    "        os.makedirs(f\"simulated_data/linked_reads/{size}/depths\", exist_ok = True)\n",
    "        DIR = f\"simulated_data/linked_reads/{size}/qc\"\n",
    "        OUTDIR = f\"simulated_data/linked_reads/{size}/depths\"\n",
    "        for num in range(1,11):\n",
    "            SAMPLE=f\"sample_{num:02}\"\n",
    "            SEED=num\n",
    "            for depth,val in depths.items():\n",
    "                executor.submit(os.system, f\"seqtk sample -s{SEED} {DIR}/{SAMPLE}.R1.fq.gz {val} | gzip > {OUTDIR}/{SAMPLE}.{depth}.R1.fq.gz\")\n",
    "                executor.submit(os.system, f\"seqtk sample -s{SEED} {DIR}/{SAMPLE}.R2.fq.gz {val} | gzip > {OUTDIR}/{SAMPLE}.{depth}.R2.fq.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning sequences\n",
    "With the simulated data _finally_ ready and QC'd, we can now align them onto the original (unmodified) 4-chromosome _D. melanogaster_ assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    DIR=\"simulated_data/linked_reads/${size}/depths\"\n",
    "    OUTDIR=\"simulated_data/alignments/${size}/\"\n",
    "    harpy align bwa --skip-reports -t 22 -o $OUTDIR -g dmel_genome/dmel_2_3.fa $DIR\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling SNPs\n",
    "After aligning, we need to call SNPs. This needs to be done per size class and per depth, since the Harpy snp calling process calls for all samples supplied at once and the output is a single BCF file of the SNPs for all the supplied samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sizes=(\"small\" \"medium\" \"large\" \"xl\")\n",
    "depths=(\"05X\" \"2X\" \"5X\" \"10X\" \"20X\")\n",
    "\n",
    "for size in \"${sizes[@]}\"; do\n",
    "    DIR=\"simulated_data/alignments/${size}/\"\n",
    "    for depth in \"${depths[@]}\"; do\n",
    "        OUTDIR=\"simulated_data/snps/${size}/${depth}\"\n",
    "        harpy snp mpileup --skip-reports -r 1000000 -t 20 -o ${OUTDIR} -g dmel_genome/dmel_2_3.fa ${DIR}/sample_*.${depth}.bam\n",
    "    done\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
